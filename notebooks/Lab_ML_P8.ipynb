{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvivst4-F_g2"
      },
      "source": [
        "# Limpieza del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyLMFQzzEbj8",
        "outputId": "bbac7392-8280-4c52-dc71-b9ef60125864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Limpieza completa: {'target': 'target', 'num_cols': ['sepal_length_(cm)', 'sepal_width_(cm)', 'petal_length_(cm)', 'petal_width_(cm)', 'target'], 'cat_cols': [], 'shape_before': (125, 5), 'shape_after': (119, 5), 'duplicates_before': 6, 'duplicates_after': 0, 'missing_before': {'sepal_length_(cm)': 2, 'sepal_width_(cm)': 2, 'petal_length_(cm)': 1, 'petal_width_(cm)': 5, 'target': 0}, 'missing_after': {'sepal_length_(cm)': 0, 'sepal_width_(cm)': 0, 'petal_length_(cm)': 0, 'petal_width_(cm)': 0, 'target': 0}, 'output': 'iris_train_clean.csv', 'report': 'cleaning_report_iris_train.md'}\n"
          ]
        }
      ],
      "source": [
        "# cleaning.py\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple, Dict\n",
        "\n",
        "TARGET_CANDIDATES = (\"species\", \"target\", \"class\")\n",
        "\n",
        "def to_snake(s: str) -> str:\n",
        "    return (\n",
        "        s.strip()\n",
        "         .lower()\n",
        "         .replace(\" \", \"_\")\n",
        "         .replace(\"-\", \"_\")\n",
        "         .replace(\".\", \"_\")\n",
        "    )\n",
        "\n",
        "def detect_target(df: pd.DataFrame, candidates: Tuple[str, ...]) -> Optional[str]:\n",
        "    for c in df.columns:\n",
        "        if c in candidates:\n",
        "            return c\n",
        "    # fallback: última columna si parece categórica\n",
        "    last = df.columns[-1]\n",
        "    if df[last].dtype == \"object\" or \"class\" in last:\n",
        "        return last\n",
        "    return None\n",
        "\n",
        "def attempt_numeric_coercion(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "    \"\"\"Intenta convertir columnas con números 'sucios' a float; conserva texto si no aplica.\"\"\"\n",
        "    num_cols, cat_cols = [], []\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[c]):\n",
        "            num_cols.append(c)\n",
        "            continue\n",
        "        # intentar coerción si hay muchos valores numéricos\n",
        "        coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        if coerced.notna().sum() > 0 and coerced.notna().sum() >= 0.8 * len(df):\n",
        "            df[c] = coerced\n",
        "            num_cols.append(c)\n",
        "        else:\n",
        "            cat_cols.append(c)\n",
        "    return df, num_cols, cat_cols\n",
        "\n",
        "def cap_iqr(s: pd.Series, k: float = 1.5) -> pd.Series:\n",
        "    q1 = s.quantile(0.25)\n",
        "    q3 = s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    low = q1 - k * iqr\n",
        "    high = q3 + k * iqr\n",
        "    return s.clip(low, high)\n",
        "\n",
        "def impute_train(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: Optional[str],\n",
        "    num_cols: List[str],\n",
        "    cat_cols: List[str],\n",
        ") -> pd.DataFrame:\n",
        "    # numéricas\n",
        "    if target_col and target_col in df.columns:\n",
        "        # asegurar que el target quede como texto (no numeric encoding aún)\n",
        "        if not pd.api.types.is_object_dtype(df[target_col]):\n",
        "            df[target_col] = df[target_col].astype(str)\n",
        "        for c in num_cols:\n",
        "            if c == target_col:\n",
        "                continue\n",
        "            if df[c].isna().any():\n",
        "                med_by_class = df.groupby(target_col)[c].transform(lambda s: s.fillna(s.median()))\n",
        "                df[c] = np.where(df[c].isna(), med_by_class, df[c])\n",
        "                if df[c].isna().any():\n",
        "                    df[c] = df[c].fillna(df[c].median())\n",
        "    else:\n",
        "        for c in num_cols:\n",
        "            df[c] = df[c].fillna(df[c].median())\n",
        "    # categóricas\n",
        "    for c in cat_cols:\n",
        "        if c == target_col:\n",
        "            continue\n",
        "        if df[c].isna().any():\n",
        "            mode = df[c].mode(dropna=True)\n",
        "            df[c] = df[c].fillna(mode.iloc[0] if len(mode) else \"missing\")\n",
        "    return df\n",
        "\n",
        "def clean_train(\n",
        "    input_csv: str | Path,\n",
        "    output_csv: str | Path,\n",
        "    report_md: str | Path,\n",
        ") -> Dict:\n",
        "    input_csv, output_csv, report_md = Path(input_csv), Path(output_csv), Path(report_md)\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # normalizar headers\n",
        "    df.columns = [to_snake(c) for c in df.columns]\n",
        "\n",
        "    # detectar objetivo\n",
        "    target_col = detect_target(df, TARGET_CANDIDATES)\n",
        "\n",
        "    # métricas iniciales\n",
        "    shape_before = df.shape\n",
        "    dtypes_before = df.dtypes.astype(str).to_dict()\n",
        "    miss_before = df.isna().sum().to_dict()\n",
        "    dup_before = int(df.duplicated().sum())\n",
        "\n",
        "    # duplicados\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    # coerción numérica\n",
        "    df, num_cols, cat_cols = attempt_numeric_coercion(df)\n",
        "\n",
        "    # imputación (train-only)\n",
        "    df = impute_train(df, target_col, num_cols, cat_cols)\n",
        "\n",
        "    # capping IQR en numéricas (no toca target)\n",
        "    for c in num_cols:\n",
        "        if c != target_col:\n",
        "            df[c] = cap_iqr(df[c])\n",
        "\n",
        "    # métricas finales\n",
        "    shape_after = df.shape\n",
        "    dup_after = int(df.duplicated().sum())\n",
        "    miss_after = df.isna().sum().to_dict()\n",
        "\n",
        "    # guardar\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "    # informe\n",
        "    lines = []\n",
        "    lines.append(\"# Informe de limpieza\\n\")\n",
        "    lines.append(f\"- **Archivo origen:** {input_csv.name}\")\n",
        "    lines.append(f\"- **Filas x columnas (antes):** {shape_before[0]} x {shape_before[1]}\")\n",
        "    lines.append(f\"- **Filas x columnas (después):** {shape_after[0]} x {shape_after[1]}\")\n",
        "    lines.append(f\"- **Duplicados (antes → después):** {dup_before} → {dup_after}\")\n",
        "    lines.append(f\"- **Tipos (antes):** {dtypes_before}\")\n",
        "    lines.append(f\"- **Faltantes (antes):** {miss_before}\")\n",
        "    lines.append(f\"- **Faltantes (después):** {miss_after}\")\n",
        "    lines.append(f\"- **Target detectado:** {target_col or '(no detectado)'}\")\n",
        "    lines.append(f\"- **Numéricas:** {', '.join([c for c in num_cols if c != target_col]) or '(ninguna)'}\")\n",
        "    lines.append(f\"- **Categóricas:** {', '.join([c for c in cat_cols if c != target_col]) or '(ninguna)'}\")\n",
        "    report_md.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "\n",
        "    return {\n",
        "        \"target\": target_col,\n",
        "        \"num_cols\": num_cols,\n",
        "        \"cat_cols\": cat_cols,\n",
        "        \"shape_before\": shape_before,\n",
        "        \"shape_after\": shape_after,\n",
        "        \"duplicates_before\": dup_before,\n",
        "        \"duplicates_after\": dup_after,\n",
        "        \"missing_before\": miss_before,\n",
        "        \"missing_after\": miss_after,\n",
        "        \"output\": str(output_csv),\n",
        "        \"report\": str(report_md),\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejemplo de uso:\n",
        "    OUT = clean_train(\n",
        "        input_csv=\"iris_train.csv\",\n",
        "        output_csv=\"iris_train_clean.csv\",\n",
        "        report_md=\"cleaning_report_iris_train.md\",\n",
        "    )\n",
        "    print(\"Limpieza completa:\", OUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ9GtlhWHBHw"
      },
      "source": [
        "random-forest-api/\n",
        "  app/\n",
        "    main.py              # TODO: FastAPI/Flask con /health, /info, /predict\n",
        "    requirements.txt\n",
        "  model/\n",
        "    rf_custom.py         # TODO: SimpleRandomForest (implementación propia)\n",
        "    rf_sklearn.py        # TODO: Wrapper de RandomForestClassifier\n",
        "    model.pkl            # placeholder vacío\n",
        "  notebooks/\n",
        "  render.yaml\n",
        "  README.md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hXXMkPauGSUl"
      },
      "outputs": [],
      "source": [
        "# TODO: implementar SimpleRandomForest (se llenará desde la API)\n",
        "class SimpleRandomForest:\n",
        "    def __init__(self, n_estimators=100, max_depth=None, random_state=42):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self._fitted = False\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"TODO: entrenar bosque propio\"\"\"\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"TODO: predicción\"\"\"\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"El modelo no está entrenado\")\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"TODO: probas (si aplica)\"\"\"\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"El modelo no está entrenado\")\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n",
        "\n",
        "# TODO: implementar wrapper de RandomForestClassifier de sklearn\n",
        "class SKRandomForest:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.params = kwargs\n",
        "        self.model = None  # se inicializa en fit\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"TODO: from sklearn.ensemble import RandomForestClassifier\"\"\"\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado\")\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado\")\n",
        "        raise NotImplementedError(\"Implementar en el equipo de API\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXmv71SIKbpt",
        "outputId": "929bc8a2-bce6-4cdd-a6b9-545db0df2dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Clasificación (Iris) ===\n",
            "Árbol único  -> Accuracy: 0.933\n",
            "SimpleRF     -> Accuracy: 0.911\n",
            "\n",
            "=== Regresión (sintético) ===\n",
            "Árbol único  -> RMSE: 103.683\n",
            "SimpleRF     -> RMSE: 73.264\n"
          ]
        }
      ],
      "source": [
        "# simple_random_forest.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Random Forest minimal (bagging + max_features por split) usando árboles de scikit-learn.\n",
        "Implementamos manualmente:\n",
        "  - bootstrap de ejemplos por árbol\n",
        "  - agregación de predicciones (mayoría / promedio)\n",
        "\n",
        "Escogemos la vía más SENCILLA y robusta:\n",
        "  - El submuestreo de variables se hace por split usando el parámetro nativo\n",
        "    `max_features` de DecisionTree*, que es lo habitual en Random Forest.\n",
        "\n",
        "Requisitos:\n",
        "    pip install numpy scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Optional, Union, List\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "class SimpleRandomForest:\n",
        "    \"\"\"\n",
        "    Parámetros\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        Nº de árboles en el ensamble.\n",
        "    max_features : {'sqrt','log2'} o int o float o None\n",
        "        Se pasa directo al DecisionTree* para submuestreo POR SPLIT.\n",
        "        - 'sqrt' recomendado en clasificación.\n",
        "        - None usa todas las variables.\n",
        "    max_depth : Optional[int]\n",
        "        Profundidad máx. de cada árbol (control de varianza).\n",
        "    random_state : Optional[int]\n",
        "        Semilla global (genera sub-semillas por árbol).\n",
        "    task : {'auto','classification','regression'}\n",
        "        'auto' infiere por el tipo de y.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_features: Union[str, int, float, None] = \"sqrt\",\n",
        "        max_depth: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        task: str = \"auto\",\n",
        "    ):\n",
        "        if n_estimators < 1:\n",
        "            raise ValueError(\"n_estimators debe ser >= 1\")\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "\n",
        "        # Atributos post-fit\n",
        "        self._rng = np.random.RandomState(self.random_state)\n",
        "        self._forest: List[Union[DecisionTreeClassifier, DecisionTreeRegressor]] = []\n",
        "        self._is_classifier: Optional[bool] = None\n",
        "        self._label_encoder: Optional[LabelEncoder] = None\n",
        "        self.n_features_in_: Optional[int] = None\n",
        "\n",
        "    # ------------------------- utilidades internas -------------------------\n",
        "\n",
        "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"Muestra bootstrap de filas con reemplazo.\"\"\"\n",
        "        n = X.shape[0]\n",
        "        idx = self._rng.randint(0, n, size=n)  # con reemplazo\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    # ---------------------------- API pública -----------------------------\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"SimpleRandomForest\":\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D (n_muestras, n_features).\")\n",
        "        if len(y.shape) != 1:\n",
        "            raise ValueError(\"y debe ser 1D (n_muestras,).\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X e y deben tener el mismo número de filas.\")\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        # Detectar tarea\n",
        "        if self.task == \"auto\":\n",
        "            # Heurística simple: floats continuos -> regresión; lo demás -> clasificación\n",
        "            self._is_classifier = not np.issubdtype(y.dtype, np.floating)\n",
        "        else:\n",
        "            if self.task not in {\"classification\", \"regression\"}:\n",
        "                raise ValueError(\"task debe ser 'auto', 'classification' o 'regression'.\")\n",
        "            self._is_classifier = (self.task == \"classification\")\n",
        "\n",
        "        # Codificación de etiquetas si clasificación\n",
        "        if self._is_classifier:\n",
        "            self._label_encoder = LabelEncoder()\n",
        "            y_fit = self._label_encoder.fit_transform(y)\n",
        "        else:\n",
        "            self._label_encoder = None\n",
        "            y_fit = y.astype(float)\n",
        "\n",
        "        # Entrenamiento con bagging\n",
        "        self._forest = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            Xb, yb = self._bootstrap_sample(X, y_fit)\n",
        "            seed = int(self._rng.randint(0, 10**9))\n",
        "\n",
        "            if self._is_classifier:\n",
        "                tree = DecisionTreeClassifier(\n",
        "                    criterion=\"gini\",\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=self.max_features,  # submuestreo POR SPLIT (estándar en RF)\n",
        "                    random_state=seed,\n",
        "                )\n",
        "            else:\n",
        "                tree = DecisionTreeRegressor(\n",
        "                    criterion=\"squared_error\",\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=self.max_features,  # submuestreo POR SPLIT\n",
        "                    random_state=seed,\n",
        "                )\n",
        "            tree.fit(Xb, yb)\n",
        "            self._forest.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if not self._forest:\n",
        "            raise RuntimeError(\"Llama a fit(X, y) antes de predict(X).\")\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D.\")\n",
        "        if self.n_features_in_ is not None and X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\n",
        "                f\"Columnas de X ({X.shape[1]}) no coinciden con las usadas en fit ({self.n_features_in_}).\"\n",
        "            )\n",
        "\n",
        "        preds = [est.predict(X).reshape(-1, 1) for est in self._forest]\n",
        "        P = np.hstack(preds)  # (n_muestras, n_estimators)\n",
        "\n",
        "        if self._is_classifier:\n",
        "            # voto mayoritario\n",
        "            n = P.shape[0]\n",
        "            votes = np.empty(n, dtype=int)\n",
        "            for i in range(n):\n",
        "                counts = np.bincount(P[i, :].astype(int))\n",
        "                votes[i] = np.argmax(counts)\n",
        "            if self._label_encoder is not None:\n",
        "                return self._label_encoder.inverse_transform(votes)\n",
        "            return votes\n",
        "        else:\n",
        "            # promedio\n",
        "            return P.mean(axis=1)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        if not self._is_classifier:\n",
        "            raise AttributeError(\"predict_proba solo aplica a clasificación.\")\n",
        "        if not self._forest:\n",
        "            raise RuntimeError(\"Llama a fit(X, y) antes de predict_proba(X).\")\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        proba = self._forest[0].predict_proba(X)\n",
        "        acc = np.zeros_like(proba)\n",
        "        acc += proba\n",
        "        for est in self._forest[1:]:\n",
        "            acc += est.predict_proba(X)\n",
        "        acc /= float(len(self._forest))\n",
        "        return acc\n",
        "\n",
        "\n",
        "# ------------------------- DEMO rápida y compatible -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import load_iris, make_regression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "    rng_seed = 42\n",
        "\n",
        "    # Clasificación: Iris\n",
        "    iris = load_iris()\n",
        "    Xc, yc = iris.data, iris.target\n",
        "    Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(\n",
        "        Xc, yc, test_size=0.30, random_state=rng_seed, stratify=yc\n",
        "    )\n",
        "\n",
        "    # Árbol único (baseline)\n",
        "    dtc = DecisionTreeClassifier(random_state=rng_seed)\n",
        "    dtc.fit(Xc_tr, yc_tr)\n",
        "    acc_tree = accuracy_score(yc_te, dtc.predict(Xc_te))\n",
        "\n",
        "    # Nuestro RF sencillo (más árboles para estabilidad en Iris)\n",
        "    srf = SimpleRandomForest(\n",
        "        n_estimators=200,\n",
        "        max_features=\"sqrt\",\n",
        "        max_depth=None,\n",
        "        random_state=rng_seed,\n",
        "        task=\"classification\",\n",
        "    ).fit(Xc_tr, yc_tr)\n",
        "    acc_rf = accuracy_score(yc_te, srf.predict(Xc_te))\n",
        "\n",
        "    print(\"=== Clasificación (Iris) ===\")\n",
        "    print(f\"Árbol único  -> Accuracy: {acc_tree:.3f}\")\n",
        "    print(f\"SimpleRF     -> Accuracy: {acc_rf:.3f}\")\n",
        "\n",
        "    # Regresión sintética\n",
        "    Xr, yr = make_regression(\n",
        "        n_samples=1200, n_features=12, n_informative=8, noise=15.0, random_state=rng_seed\n",
        "    )\n",
        "    Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(Xr, yr, test_size=0.30, random_state=rng_seed)\n",
        "\n",
        "    dtr = DecisionTreeRegressor(random_state=rng_seed)\n",
        "    dtr.fit(Xr_tr, yr_tr)\n",
        "    rmse_tree = float(np.sqrt(mean_squared_error(yr_te, dtr.predict(Xr_te))))\n",
        "\n",
        "    srf_reg = SimpleRandomForest(\n",
        "        n_estimators=200,\n",
        "        max_features=\"sqrt\",\n",
        "        max_depth=None,\n",
        "        random_state=rng_seed,\n",
        "        task=\"regression\",\n",
        "    ).fit(Xr_tr, yr_tr)\n",
        "    rmse_rf = float(np.sqrt(mean_squared_error(yr_te, srf_reg.predict(Xr_te))))\n",
        "\n",
        "    print(\"\\n=== Regresión (sintético) ===\")\n",
        "    print(f\"Árbol único  -> RMSE: {rmse_tree:.3f}\")\n",
        "    print(f\"SimpleRF     -> RMSE: {rmse_rf:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d7b4d63",
        "outputId": "6862b424-fefb-43dd-d213-b856531d4d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of SimpleRandomForest on cleaned data: 0.917\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('iris_train_clean.csv')\n",
        "\n",
        "# Assuming the target column is 'target' as detected in the cleaning step\n",
        "TARGET_COLUMN = 'target'\n",
        "\n",
        "# Prepare data for the model\n",
        "X = df_cleaned.drop(columns=[TARGET_COLUMN])\n",
        "y = df_cleaned[TARGET_COLUMN]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=rng_seed, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train the SimpleRandomForest model\n",
        "srf_cleaned = SimpleRandomForest(\n",
        "    n_estimators=200,\n",
        "    max_features=\"sqrt\",\n",
        "    max_depth=None,\n",
        "    random_state=rng_seed,\n",
        "    task=\"classification\",\n",
        ")\n",
        "srf_cleaned.fit(X_train.values, y_train.values) # Convert to numpy arrays for SimpleRandomForest\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = srf_cleaned.predict(X_test.values) # Convert to numpy array\n",
        "accuracy_cleaned = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of SimpleRandomForest on cleaned data: {accuracy_cleaned:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "710e23e7",
        "outputId": "bf6fcc10-24f3-481d-bf49-68f8b7585e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of scikit-learn RandomForestClassifier on cleaned data: 0.917\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize and train the scikit-learn RandomForestClassifier model\n",
        "sklearn_rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_features=\"sqrt\",\n",
        "    max_depth=None,\n",
        "    random_state=rng_seed,\n",
        ")\n",
        "\n",
        "sklearn_rf.fit(X_train, y_train) # scikit-learn models can handle pandas DataFrames directly\n",
        "\n",
        "# Evaluate the scikit-learn model\n",
        "y_pred_sklearn = sklearn_rf.predict(X_test)\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(f\"Accuracy of scikit-learn RandomForestClassifier on cleaned data: {accuracy_sklearn:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5877b925"
      },
      "source": [
        "### Comparación de Modelos\n",
        "\n",
        "Hemos entrenado y evaluado dos modelos de Random Forest en los datos limpios del dataset Iris:\n",
        "\n",
        "1.  **SimpleRandomForest (implementación propia):** Obtuvimos una precisión (accuracy) de **{{accuracy_cleaned:.3f}}**.\n",
        "2.  **scikit-learn RandomForestClassifier:** Obtuvimos una precisión (accuracy) de **{{accuracy_sklearn:.3f}}**.\n",
        "\n",
        "En este caso particular y con los hiperparámetros utilizados, ambos modelos lograron la misma precisión en el conjunto de prueba. Esto sugiere que tu implementación `SimpleRandomForest` se comporta de manera muy similar a la versión estándar de scikit-learn para este dataset.\n",
        "\n",
        "Consideraciones:\n",
        "*   **Complejidad:** La implementación de scikit-learn es mucho más robusta, optimizada y probada, con soporte para más funcionalidades y tipos de datos.\n",
        "*   **Personalización:** Tu `SimpleRandomForest` te da control total sobre el proceso, lo cual es útil para entender los fundamentos o implementar variaciones específicas.\n",
        "*   **Rendimiento:** Para datasets grandes o entrenamientos intensivos, la versión de scikit-learn probablemente será más eficiente.\n",
        "\n",
        "Para una comparación más profunda, se podrían analizar otras métricas como precisión, recall, F1-score, o la curva ROC (para clasificación binaria), así como el tiempo de entrenamiento y predicción."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env (3.12.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
