{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ_uHyikS-ZC"
      },
      "source": [
        "#rf_custom.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oq5VIMPkOOoi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# simple_random_forest.py\n",
        "# Implementación sencilla de un Random Forest propio usando árboles de scikit-learn.\n",
        "# Solo la clase con fit / predict (y predict_proba para clasificación).\n",
        "\n",
        "from typing import Optional, Union\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "class SimpleRandomForest:\n",
        "    \"\"\"\n",
        "    Random Forest minimalista (bagging de árboles) con interfaz estilo scikit-learn.\n",
        "\n",
        "    - Usa bootstrap de ejemplos para cada árbol.\n",
        "    - Usa submuestreo aleatorio de características por división a través de `max_features`\n",
        "      (delegado al árbol base de scikit-learn).\n",
        "    - Agregación:\n",
        "        * Clasificación: promedio de probabilidades y argmax.\n",
        "        * Regresión: promedio de predicciones.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    n_estimators : int, default=100\n",
        "        Número de árboles en el ensamble.\n",
        "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
        "        Número de características a considerar en cada división del árbol base.\n",
        "        Se pasa directo al árbol (traducción: 'all' -> None).\n",
        "    criterion : str, default='gini' (clasificación) o 'squared_error' (regresión)\n",
        "        Criterio del árbol. Si task='auto', se ignora este valor y se elige por tipo.\n",
        "    max_depth : int o None, default=None\n",
        "        Profundidad máxima de cada árbol.\n",
        "    random_state : int o None, default=None\n",
        "        Semilla global para reproducibilidad.\n",
        "    task : {'auto','classification','regression'}, default='auto'\n",
        "        Tipo de problema. Si 'auto', se infiere a partir de y.\n",
        "\n",
        "    Atributos (tras fit)\n",
        "    --------------------\n",
        "    estimators_ : list\n",
        "        Lista de árboles entrenados.\n",
        "    task_ : str\n",
        "        Tipo inferido: 'classification' o 'regression'.\n",
        "    classes_ : np.ndarray (solo clasificación)\n",
        "        Clases en el orden usado para agregación.\n",
        "    n_features_in_ : int\n",
        "        Número de características de entrada.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_features: Union[str, int, float] = \"sqrt\",\n",
        "        criterion: Optional[str] = None,\n",
        "        max_depth: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        task: str = \"auto\",\n",
        "    ):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "\n",
        "        # Set en fit\n",
        "        self.estimators_ = []\n",
        "        self.task_ = None\n",
        "        self.classes_ = None\n",
        "        self._le = None\n",
        "        self.n_features_in_ = None\n",
        "        self._rng = None\n",
        "\n",
        "    # --------------------------\n",
        "    # Utilidades internas\n",
        "    # --------------------------\n",
        "    def _resolve_max_features(self, n_features: int):\n",
        "        \"\"\"Traduce el argumento max_features a lo que esperan los árboles de sklearn.\"\"\"\n",
        "        mf = self.max_features\n",
        "        if mf == \"all\":\n",
        "            return None  # sklearn: None => usa todas\n",
        "        if isinstance(mf, str):\n",
        "            if mf in {\"sqrt\", \"log2\"}:\n",
        "                return mf\n",
        "            raise ValueError(\"max_features string debe ser 'sqrt', 'log2' o 'all'.\")\n",
        "        if isinstance(mf, (int, np.integer)):\n",
        "            if 1 <= mf <= n_features:\n",
        "                return int(mf)\n",
        "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
        "        if isinstance(mf, float):\n",
        "            if 0.0 < mf <= 1.0:\n",
        "                k = max(1, int(np.floor(mf * n_features)))\n",
        "                return k\n",
        "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
        "        # None no es válido aquí (usar 'all' para todas)\n",
        "        raise ValueError(\"max_features no válido.\")\n",
        "\n",
        "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray, rng: np.random.RandomState):\n",
        "        \"\"\"Devuelve una muestra bootstrap de (X, y) del mismo tamaño que el conjunto original.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        idx = rng.randint(0, n_samples, size=n_samples)  # con reemplazo\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def _infer_task(self, y: np.ndarray) -> str:\n",
        "        if self.task in (\"classification\", \"regression\"):\n",
        "            return self.task\n",
        "        y_type = type_of_target(y)\n",
        "        if y_type in (\"binary\", \"multiclass\"):\n",
        "            return \"classification\"\n",
        "        elif y_type in (\"continuous\",):\n",
        "            return \"regression\"\n",
        "        else:\n",
        "            # Heurística simple: si y es entera -> clasificación, si no -> regresión\n",
        "            return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
        "\n",
        "    # --------------------------\n",
        "    # API principal\n",
        "    # --------------------------\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.task_ = self._infer_task(y)\n",
        "        self._rng = np.random.RandomState(self.random_state)\n",
        "\n",
        "        max_features = self._resolve_max_features(self.n_features_in_)\n",
        "\n",
        "        self.estimators_ = []\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            # Aseguramos un orden global de clases para la agregación\n",
        "            self._le = LabelEncoder().fit(y)\n",
        "            self.classes_ = self._le.classes_\n",
        "\n",
        "            # Criterio por defecto si no se especifica\n",
        "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
        "\n",
        "            for _ in range(self.n_estimators):\n",
        "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
        "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
        "                tree = DecisionTreeClassifier(\n",
        "                    criterion=criterion,\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=max_features,\n",
        "                    random_state=seed,\n",
        "                )\n",
        "                tree.fit(Xi, yi)\n",
        "                self.estimators_.append(tree)\n",
        "\n",
        "        else:  # regresión\n",
        "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
        "\n",
        "            for _ in range(self.n_estimators):\n",
        "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
        "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
        "                tree = DecisionTreeRegressor(\n",
        "                    criterion=criterion,\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=max_features,\n",
        "                    random_state=seed,\n",
        "                )\n",
        "                tree.fit(Xi, yi)\n",
        "                self.estimators_.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if not self.estimators_:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "\n",
        "        X = np.asarray(X)\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            # Promedio de probabilidades con alineación de clases\n",
        "            proba = self.predict_proba(X)\n",
        "            # argmax sobre el eje de clases y deshacer codificación\n",
        "            y_ind = np.argmax(proba, axis=1)\n",
        "            return self.classes_[y_ind]\n",
        "        else:\n",
        "            # Promedio de predicciones de regresión\n",
        "            preds = np.column_stack([est.predict(X) for est in self.estimators_])\n",
        "            return preds.mean(axis=1)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Probabilidades promedio para clasificación. No disponible para regresión.\"\"\"\n",
        "        if self.task_ != \"classification\":\n",
        "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes_)\n",
        "        proba_sum = np.zeros((n_samples, n_classes), dtype=float)\n",
        "\n",
        "        for est in self.estimators_:\n",
        "            # Probabilidades del árbol actual (puede faltar alguna clase en el bootstrap)\n",
        "            est_proba = est.predict_proba(X)  # (n_samples, n_classes_est)\n",
        "            est_classes = est.classes_         # clases vistas por este árbol\n",
        "            # Mapear columnas a índice global de clases\n",
        "            # Dado que LabelEncoder ordena y las clases_ del árbol también están ordenadas,\n",
        "            # podemos ubicar con searchsorted.\n",
        "            idx_map = np.searchsorted(self.classes_, est_classes)\n",
        "            # Sumar en las columnas correspondientes\n",
        "            proba_sum[:, idx_map] += est_proba\n",
        "\n",
        "        # Promedio\n",
        "        proba_avg = proba_sum / float(len(self.estimators_))\n",
        "\n",
        "        # Normalización por si algún árbol omitió clases (para robustez numérica)\n",
        "        row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
        "        # Evitar división por cero: si alguna fila suma 0 (muy raro), asignar uniforme\n",
        "        zero_rows = (row_sums[:, 0] == 0.0)\n",
        "        if np.any(zero_rows):\n",
        "            proba_avg[zero_rows, :] = 1.0 / n_classes\n",
        "            row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
        "        proba_avg /= row_sums\n",
        "\n",
        "        return proba_avg\n",
        "\n",
        "    # Compatibilidad mínima con API sklearn\n",
        "    def get_params(self, deep: bool = True):\n",
        "        return {\n",
        "            \"n_estimators\": self.n_estimators,\n",
        "            \"max_features\": self.max_features,\n",
        "            \"criterion\": self.criterion,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"random_state\": self.random_state,\n",
        "            \"task\": self.task,\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for k, v in params.items():\n",
        "            setattr(self, k, v)\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JubLqn9TEe1"
      },
      "source": [
        "##rf_sklearn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kk_3vLymS6dj"
      },
      "outputs": [],
      "source": [
        "# simple_random_forest_sklearn.py\n",
        "# Implementación mínima de Random Forest usando directamente scikit-learn.\n",
        "# Mantiene una API similar a la versión \"propia\": fit / predict / predict_proba.\n",
        "\n",
        "from typing import Optional, Union\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "\n",
        "\n",
        "class SimpleRandomForestSK:\n",
        "    \"\"\"\n",
        "    Envoltura ligera sobre RandomForest de scikit-learn con una interfaz simple y consistente.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    n_estimators : int, default=100\n",
        "        Número de árboles.\n",
        "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
        "        Subconjunto de características por división. 'all' -> usa todas (None en sklearn).\n",
        "    criterion : str, default='gini' (clasificación) o 'squared_error' (regresión)\n",
        "        Criterio del árbol base.\n",
        "    max_depth : int o None, default=None\n",
        "        Profundidad máxima de cada árbol.\n",
        "    random_state : int o None, default=None\n",
        "        Semilla de aleatoriedad.\n",
        "    task : {'auto','classification','regression'}, default='auto'\n",
        "        Tipo de problema; si 'auto', se infiere de y.\n",
        "\n",
        "    Atributos (tras fit)\n",
        "    --------------------\n",
        "    estimator_ : RandomForestClassifier o RandomForestRegressor\n",
        "        Modelo entrenado.\n",
        "    task_ : str\n",
        "        'classification' o 'regression'.\n",
        "    classes_ : np.ndarray (solo clasificación)\n",
        "        Clases del modelo.\n",
        "    n_features_in_ : int\n",
        "        Número de columnas de X usadas en el entrenamiento.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_features: Union[str, int, float] = \"sqrt\",\n",
        "        criterion: Optional[str] = None,\n",
        "        max_depth: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        task: str = \"auto\",\n",
        "    ):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "\n",
        "        self.estimator_ = None\n",
        "        self.task_ = None\n",
        "        self.classes_ = None\n",
        "        self.n_features_in_ = None\n",
        "\n",
        "    # --------------------------\n",
        "    # Utilidades internas\n",
        "    # --------------------------\n",
        "    def _resolve_max_features(self, n_features: int):\n",
        "        \"\"\"Traduce 'all' a None y valida enteros/floats dentro de rango.\"\"\"\n",
        "        mf = self.max_features\n",
        "        if mf == \"all\":\n",
        "            return None\n",
        "        if isinstance(mf, (int, np.integer)):\n",
        "            if 1 <= mf <= n_features:\n",
        "                return int(mf)\n",
        "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
        "        if isinstance(mf, float):\n",
        "            if 0.0 < mf <= 1.0:\n",
        "                return float(mf)\n",
        "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
        "        if mf in (None, \"sqrt\", \"log2\"):\n",
        "            return mf\n",
        "        raise ValueError(\"max_features debe ser 'sqrt', 'log2', 'all', None, int o float en (0,1].\")\n",
        "\n",
        "    def _infer_task(self, y: np.ndarray) -> str:\n",
        "        if self.task in (\"classification\", \"regression\"):\n",
        "            return self.task\n",
        "        y_type = type_of_target(y)\n",
        "        if y_type in (\"binary\", \"multiclass\"):\n",
        "            return \"classification\"\n",
        "        elif y_type in (\"continuous\",):\n",
        "            return \"regression\"\n",
        "        # fallback heurístico\n",
        "        return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
        "\n",
        "    # --------------------------\n",
        "    # API principal\n",
        "    # --------------------------\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.task_ = self._infer_task(y)\n",
        "        max_features = self._resolve_max_features(self.n_features_in_)\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
        "            self.estimator_ = RandomForestClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                max_features=max_features,\n",
        "                criterion=criterion,\n",
        "                bootstrap=True,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        else:\n",
        "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
        "            self.estimator_ = RandomForestRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                max_features=max_features,\n",
        "                criterion=criterion,\n",
        "                bootstrap=True,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "\n",
        "        self.estimator_.fit(X, y)\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            self.classes_ = self.estimator_.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.estimator_ is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "        X = np.asarray(X)\n",
        "        return self.estimator_.predict(X)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.estimator_ is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "        if self.task_ != \"classification\":\n",
        "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
        "        X = np.asarray(X)\n",
        "        return self.estimator_.predict_proba(X)\n",
        "\n",
        "    # Compatibilidad mínima con API sklearn\n",
        "    def get_params(self, deep: bool = True):\n",
        "        return {\n",
        "            \"n_estimators\": self.n_estimators,\n",
        "            \"max_features\": self.max_features,\n",
        "            \"criterion\": self.criterion,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"random_state\": self.random_state,\n",
        "            \"task\": self.task,\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for k, v in params.items():\n",
        "            setattr(self, k, v)\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE_0MWauTKtK"
      },
      "source": [
        "# model.pkl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdU8izMiTQNY",
        "outputId": "076f4943-f513-4f32-abe7-465abc37f4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SimpleRandomForest (propio) en Iris (clean) ===\n",
            "Accuracy: 0.889\n",
            "\n",
            "Clases (orden interno): [0 1 2]\n",
            "Matriz de confusión:\n",
            " [[12  0  0]\n",
            " [ 0 10  2]\n",
            " [ 0  2 10]]\n",
            "\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000        12\n",
            "           1      0.833     0.833     0.833        12\n",
            "           2      0.833     0.833     0.833        12\n",
            "\n",
            "    accuracy                          0.889        36\n",
            "   macro avg      0.889     0.889     0.889        36\n",
            "weighted avg      0.889     0.889     0.889        36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train_srf_iris_standalone.py\n",
        "# Entrena y evalúa un Random Forest propio (bagging de árboles) sobre iris_train_clean.csv\n",
        "# Todo en un solo archivo: clase + entrenamiento.\n",
        "\n",
        "from typing import Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "class SimpleRandomForest:\n",
        "    \"\"\"\n",
        "    Random Forest minimalista (bagging de árboles) con interfaz estilo scikit-learn.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    n_estimators : int, default=100\n",
        "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
        "    criterion : str, default='gini' (clasif) o 'squared_error' (regresión)\n",
        "    max_depth : int o None\n",
        "    random_state : int o None\n",
        "    task : {'auto','classification','regression'}\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_features: Union[str, int, float] = \"sqrt\",\n",
        "        criterion: Optional[str] = None,\n",
        "        max_depth: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        task: str = \"auto\",\n",
        "    ):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "\n",
        "        self.estimators_ = []\n",
        "        self.task_ = None\n",
        "        self.classes_ = None\n",
        "        self._le = None\n",
        "        self.n_features_in_ = None\n",
        "        self._rng = None\n",
        "\n",
        "    # -------- utilidades internas --------\n",
        "    def _resolve_max_features(self, n_features: int):\n",
        "        mf = self.max_features\n",
        "        if mf == \"all\":\n",
        "            return None  # sklearn usa None para \"todas\"\n",
        "        if isinstance(mf, str):\n",
        "            if mf in {\"sqrt\", \"log2\"}:\n",
        "                return mf\n",
        "            raise ValueError(\"max_features string debe ser 'sqrt', 'log2' o 'all'.\")\n",
        "        if isinstance(mf, (int, np.integer)):\n",
        "            if 1 <= mf <= n_features:\n",
        "                return int(mf)\n",
        "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
        "        if isinstance(mf, float):\n",
        "            if 0.0 < mf <= 1.0:\n",
        "                return max(1, int(np.floor(mf * n_features)))\n",
        "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
        "        raise ValueError(\"max_features no válido.\")\n",
        "\n",
        "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray, rng: np.random.RandomState):\n",
        "        n_samples = X.shape[0]\n",
        "        idx = rng.randint(0, n_samples, size=n_samples)  # con reemplazo\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def _infer_task(self, y: np.ndarray) -> str:\n",
        "        if self.task in (\"classification\", \"regression\"):\n",
        "            return self.task\n",
        "        y_type = type_of_target(y)\n",
        "        if y_type in (\"binary\", \"multiclass\"):\n",
        "            return \"classification\"\n",
        "        elif y_type in (\"continuous\",):\n",
        "            return \"regression\"\n",
        "        return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
        "\n",
        "    # -------------- API --------------\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.task_ = self._infer_task(y)\n",
        "        self._rng = np.random.RandomState(self.random_state)\n",
        "        max_features = self._resolve_max_features(self.n_features_in_)\n",
        "        self.estimators_ = []\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            self._le = LabelEncoder().fit(y)\n",
        "            self.classes_ = self._le.classes_\n",
        "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
        "            for _ in range(self.n_estimators):\n",
        "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
        "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
        "                tree = DecisionTreeClassifier(\n",
        "                    criterion=criterion,\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=max_features,\n",
        "                    random_state=seed,\n",
        "                )\n",
        "                tree.fit(Xi, yi)\n",
        "                self.estimators_.append(tree)\n",
        "        else:\n",
        "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
        "            for _ in range(self.n_estimators):\n",
        "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
        "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
        "                tree = DecisionTreeRegressor(\n",
        "                    criterion=criterion,\n",
        "                    max_depth=self.max_depth,\n",
        "                    max_features=max_features,\n",
        "                    random_state=seed,\n",
        "                )\n",
        "                tree.fit(Xi, yi)\n",
        "                self.estimators_.append(tree)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if not self.estimators_:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "        X = np.asarray(X)\n",
        "        if self.task_ == \"classification\":\n",
        "            proba = self.predict_proba(X)\n",
        "            y_ind = np.argmax(proba, axis=1)\n",
        "            return self.classes_[y_ind]\n",
        "        preds = np.column_stack([est.predict(X) for est in self.estimators_])\n",
        "        return preds.mean(axis=1)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.task_ != \"classification\":\n",
        "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes_)\n",
        "        proba_sum = np.zeros((n_samples, n_classes), dtype=float)\n",
        "        for est in self.estimators_:\n",
        "            est_proba = est.predict_proba(X)\n",
        "            est_classes = est.classes_\n",
        "            idx_map = np.searchsorted(self.classes_, est_classes)\n",
        "            proba_sum[:, idx_map] += est_proba\n",
        "        proba_avg = proba_sum / float(len(self.estimators_))\n",
        "        row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
        "        zero_rows = (row_sums[:, 0] == 0.0)\n",
        "        if np.any(zero_rows):\n",
        "            proba_avg[zero_rows, :] = 1.0 / n_classes\n",
        "            row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
        "        proba_avg /= row_sums\n",
        "        return proba_avg\n",
        "\n",
        "    def get_params(self, deep: bool = True):\n",
        "        return {\n",
        "            \"n_estimators\": self.n_estimators,\n",
        "            \"max_features\": self.max_features,\n",
        "            \"criterion\": self.criterion,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"random_state\": self.random_state,\n",
        "            \"task\": self.task,\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for k, v in params.items():\n",
        "            setattr(self, k, v)\n",
        "        return self\n",
        "\n",
        "\n",
        "# ======= Entrenamiento simple en Iris limpio =======\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_PATH = \"iris_train_clean.csv\"  # ajusta si tu ruta es distinta\n",
        "    TARGET_COLUMN = \"target\"\n",
        "    RNG_SEED = 42\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    if TARGET_COLUMN not in df.columns:\n",
        "        raise ValueError(f\"No existe la columna objetivo '{TARGET_COLUMN}' en el CSV.\")\n",
        "\n",
        "    X = df.drop(columns=[TARGET_COLUMN]).values\n",
        "    y = df[TARGET_COLUMN].values\n",
        "\n",
        "    # Por si vienen como 0.0,1.0,2.0 -> int\n",
        "    if y.dtype.kind == \"f\":\n",
        "        import numpy as _np\n",
        "        if _np.allclose(y, y.round()):\n",
        "            y = y.round().astype(int)\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=RNG_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    srf = SimpleRandomForest(\n",
        "        n_estimators=200,\n",
        "        max_features=\"sqrt\",\n",
        "        max_depth=None,\n",
        "        random_state=RNG_SEED,\n",
        "        task=\"classification\",\n",
        "    ).fit(X_tr, y_tr)\n",
        "\n",
        "    y_pred = srf.predict(X_te)\n",
        "    acc = accuracy_score(y_te, y_pred)\n",
        "    cm = confusion_matrix(y_te, y_pred, labels=srf.classes_)\n",
        "    report = classification_report(y_te, y_pred, digits=3)\n",
        "\n",
        "    print(\"=== SimpleRandomForest (propio) en Iris (clean) ===\")\n",
        "    print(f\"Accuracy: {acc:.3f}\")\n",
        "    print(\"\\nClases (orden interno):\", srf.classes_)\n",
        "    print(\"Matriz de confusión:\\n\", cm)\n",
        "    print(\"\\nReporte de clasificación:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgb32VGvVJSR",
        "outputId": "37e6ca2b-ea7e-4ed5-a3c9-d1c23824072c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SimpleRandomForest (propio) model saved to ../model/srf_propio_model.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Define the filename for the pickle file\n",
        "model_filename = '../model/srf_propio_model.pkl' # Changed filename to be more specific\n",
        "\n",
        "# Save the trained SimpleRandomForest model (propio) to a pickle file\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(srf, f) # Corrected variable name from srf_cleaned to srf\n",
        "\n",
        "print(f\"SimpleRandomForest (propio) model saved to {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyocbEGQUUe5"
      },
      "source": [
        "# Comparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pzJZr7AUnnz",
        "outputId": "99f46fdf-2ac8-4c18-90b3-08569a16fd61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== RandomForest (sklearn wrapper) en Iris (clean) ===\n",
            "Accuracy: 0.917\n",
            "\n",
            "Clases (orden interno): [0 1 2]\n",
            "Matriz de confusión:\n",
            " [[12  0  0]\n",
            " [ 0 10  2]\n",
            " [ 0  1 11]]\n",
            "\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000        12\n",
            "           1      0.909     0.833     0.870        12\n",
            "           2      0.846     0.917     0.880        12\n",
            "\n",
            "    accuracy                          0.917        36\n",
            "   macro avg      0.918     0.917     0.917        36\n",
            "weighted avg      0.918     0.917     0.917        36\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train_srf_sklearn_standalone.py\n",
        "# Entrena y evalúa un Random Forest usando scikit-learn (wrapper SimpleRandomForestSK).\n",
        "# Todo en un solo archivo para evitar problemas de import.\n",
        "\n",
        "from typing import Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "class SimpleRandomForestSK:\n",
        "    \"\"\"\n",
        "    Envoltura ligera sobre RandomForest de scikit-learn con una interfaz simple y consistente.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    n_estimators : int, default=100\n",
        "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
        "        'all' -> usa todas (None en sklearn).\n",
        "    criterion : str, default='gini' (clasificación) o 'squared_error' (regresión)\n",
        "    max_depth : int o None, default=None\n",
        "    random_state : int o None, default=None\n",
        "    task : {'auto','classification','regression'}, default='auto'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_features: Union[str, int, float] = \"sqrt\",\n",
        "        criterion: Optional[str] = None,\n",
        "        max_depth: Optional[int] = None,\n",
        "        random_state: Optional[int] = None,\n",
        "        task: str = \"auto\",\n",
        "    ):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "\n",
        "        self.estimator_ = None\n",
        "        self.task_ = None\n",
        "        self.classes_ = None\n",
        "        self.n_features_in_ = None\n",
        "\n",
        "    # --------------------------\n",
        "    # Utilidades internas\n",
        "    # --------------------------\n",
        "    def _resolve_max_features(self, n_features: int):\n",
        "        \"\"\"Traduce 'all' a None y valida enteros/floats dentro de rango.\"\"\"\n",
        "        mf = self.max_features\n",
        "        if mf == \"all\":\n",
        "            return None\n",
        "        if isinstance(mf, (int, np.integer)):\n",
        "            if 1 <= mf <= n_features:\n",
        "                return int(mf)\n",
        "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
        "        if isinstance(mf, float):\n",
        "            if 0.0 < mf <= 1.0:\n",
        "                return float(mf)\n",
        "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
        "        if mf in (None, \"sqrt\", \"log2\"):\n",
        "            return mf\n",
        "        raise ValueError(\"max_features debe ser 'sqrt', 'log2', 'all', None, int o float en (0,1].\")\n",
        "\n",
        "    def _infer_task(self, y: np.ndarray) -> str:\n",
        "        if self.task in (\"classification\", \"regression\"):\n",
        "            return self.task\n",
        "        y_type = type_of_target(y)\n",
        "        if y_type in (\"binary\", \"multiclass\"):\n",
        "            return \"classification\"\n",
        "        elif y_type in (\"continuous\",):\n",
        "            return \"regression\"\n",
        "        return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
        "\n",
        "    # --------------------------\n",
        "    # API principal\n",
        "    # --------------------------\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.task_ = self._infer_task(y)\n",
        "        max_features = self._resolve_max_features(self.n_features_in_)\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
        "            self.estimator_ = RandomForestClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                max_features=max_features,\n",
        "                criterion=criterion,\n",
        "                bootstrap=True,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        else:\n",
        "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
        "            self.estimator_ = RandomForestRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                max_depth=self.max_depth,\n",
        "                max_features=max_features,\n",
        "                criterion=criterion,\n",
        "                bootstrap=True,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "\n",
        "        self.estimator_.fit(X, y)\n",
        "\n",
        "        if self.task_ == \"classification\":\n",
        "            self.classes_ = self.estimator_.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.estimator_ is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "        X = np.asarray(X)\n",
        "        return self.estimator_.predict(X)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        if self.estimator_ is None:\n",
        "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
        "        if self.task_ != \"classification\":\n",
        "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
        "        X = np.asarray(X)\n",
        "        return self.estimator_.predict_proba(X)\n",
        "\n",
        "    # Compatibilidad mínima con API sklearn\n",
        "    def get_params(self, deep: bool = True):\n",
        "        return {\n",
        "            \"n_estimators\": self.n_estimators,\n",
        "            \"max_features\": self.max_features,\n",
        "            \"criterion\": self.criterion,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"random_state\": self.random_state,\n",
        "            \"task\": self.task,\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for k, v in params.items():\n",
        "            setattr(self, k, v)\n",
        "        return self\n",
        "\n",
        "\n",
        "# ======= Entrenamiento/Evaluación en Iris limpio =======\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_PATH = \"iris_train_clean.csv\"  # ajusta la ruta si es necesario\n",
        "    TARGET_COLUMN = \"target\"\n",
        "    RNG_SEED = 42\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    if TARGET_COLUMN not in df.columns:\n",
        "        raise ValueError(f\"No existe la columna objetivo '{TARGET_COLUMN}' en el CSV.\")\n",
        "\n",
        "    X = df.drop(columns=[TARGET_COLUMN]).values\n",
        "    y = df[TARGET_COLUMN].values\n",
        "\n",
        "    # Si vienen etiquetas 0.0,1.0,2.0 como float, castear a int\n",
        "    if y.dtype.kind == \"f\" and np.allclose(y, y.round()):\n",
        "        y = y.round().astype(int)\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=RNG_SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    rf = SimpleRandomForestSK(\n",
        "        n_estimators=200,\n",
        "        max_features=\"sqrt\",     # típico en clasificación\n",
        "        max_depth=None,          # árboles profundos\n",
        "        random_state=RNG_SEED,\n",
        "        task=\"classification\",\n",
        "    ).fit(X_tr, y_tr)\n",
        "\n",
        "    y_pred = rf.predict(X_te)\n",
        "    acc = accuracy_score(y_te, y_pred)\n",
        "    cm = confusion_matrix(y_te, y_pred, labels=rf.classes_)\n",
        "    report = classification_report(y_te, y_pred, digits=3)\n",
        "\n",
        "    print(\"=== RandomForest (sklearn wrapper) en Iris (clean) ===\")\n",
        "    print(f\"Accuracy: {acc:.3f}\")\n",
        "    print(\"\\nClases (orden interno):\", rf.classes_)\n",
        "    print(\"Matriz de confusión:\\n\", cm)\n",
        "    print(\"\\nReporte de clasificación:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNkD1iq-UoBM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env (3.12.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
