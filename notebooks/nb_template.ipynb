{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848153e3",
   "metadata": {},
   "source": [
    "# Laboratorio 8: Random Forest y despliegues\n",
    "\n",
    "**Duración:** 2 horas  \n",
    "**Formato:** Implementación, despliegue y competencia  \n",
    "\n",
    "---\n",
    "\n",
    "## Portada del equipo\n",
    "\n",
    "**Integrantes:**\n",
    "- Alanís González Sebástian\n",
    "- Arano Bejarano Melisa Asharet \n",
    "- Fonseca González Bruno \n",
    "- Morales Flores Luis Enrique\n",
    "\n",
    "**Repositorio del equipo:**  \n",
    "<https://github.com/lukemorales13/random-forest-api>\n",
    "\n",
    "**Fecha de entrega:**  \n",
    "31/Octubre/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067997e4",
   "metadata": {},
   "source": [
    "## Elemento 1 - Implementación del Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12dcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class SimpleRandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest minimalista (bagging de árboles) con interfaz estilo scikit-learn.\n",
    "\n",
    "    - Usa bootstrap de ejemplos para cada árbol.\n",
    "    - Usa submuestreo aleatorio de características por división a través de `max_features`\n",
    "      (delegado al árbol base de scikit-learn).\n",
    "    - Agregación:\n",
    "        * Clasificación: promedio de probabilidades y argmax.\n",
    "        * Regresión: promedio de predicciones.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    n_estimators : int, default=100\n",
    "        Número de árboles en el ensamble.\n",
    "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
    "        Número de características a considerar en cada división del árbol base.\n",
    "        Se pasa directo al árbol (traducción: 'all' -> None).\n",
    "    criterion : str, default='gini' (clasificación) o 'squared_error' (regresión)\n",
    "        Criterio del árbol. Si task='auto', se ignora este valor y se elige por tipo.\n",
    "    max_depth : int o None, default=None\n",
    "        Profundidad máxima de cada árbol.\n",
    "    random_state : int o None, default=None\n",
    "        Semilla global para reproducibilidad.\n",
    "    task : {'auto','classification','regression'}, default='auto'\n",
    "        Tipo de problema. Si 'auto', se infiere a partir de y.\n",
    "\n",
    "    Atributos (tras fit)\n",
    "    --------------------\n",
    "    estimators_ : list\n",
    "        Lista de árboles entrenados.\n",
    "    task_ : str\n",
    "        Tipo inferido: 'classification' o 'regression'.\n",
    "    classes_ : np.ndarray (solo clasificación)\n",
    "        Clases en el orden usado para agregación.\n",
    "    n_features_in_ : int\n",
    "        Número de características de entrada.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        max_features: Union[str, int, float] = \"sqrt\",\n",
    "        criterion: Optional[str] = None,\n",
    "        max_depth: Optional[int] = None,\n",
    "        random_state: Optional[int] = None,\n",
    "        task: str = \"auto\",\n",
    "    ):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.task = task\n",
    "\n",
    "        # Set en fit\n",
    "        self.estimators_ = []\n",
    "        self.task_ = None\n",
    "        self.classes_ = None\n",
    "        self._le = None\n",
    "        self.n_features_in_ = None\n",
    "        self._rng = None\n",
    "\n",
    "    # Utilidades internas\n",
    "    def _resolve_max_features(self, n_features: int):\n",
    "        \"\"\"Traduce el argumento max_features a lo que esperan los árboles de sklearn.\"\"\"\n",
    "        mf = self.max_features\n",
    "        if mf == \"all\":\n",
    "            return None  # sklearn: None => usa todas\n",
    "        if isinstance(mf, str):\n",
    "            if mf in {\"sqrt\", \"log2\"}:\n",
    "                return mf\n",
    "            raise ValueError(\"max_features string debe ser 'sqrt', 'log2' o 'all'.\")\n",
    "        if isinstance(mf, (int, np.integer)):\n",
    "            if 1 <= mf <= n_features:\n",
    "                return int(mf)\n",
    "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
    "        if isinstance(mf, float):\n",
    "            if 0.0 < mf <= 1.0:\n",
    "                k = max(1, int(np.floor(mf * n_features)))\n",
    "                return k\n",
    "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
    "        raise ValueError(\"max_features no válido.\")\n",
    "    # Muestra bootstrap\n",
    "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray, rng: np.random.RandomState):\n",
    "        \"\"\"Devuelve una muestra bootstrap de (X, y) del mismo tamaño que el conjunto original.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        idx = rng.randint(0, n_samples, size=n_samples)  # con reemplazo\n",
    "        return X[idx], y[idx]\n",
    "    # Inferencia de tarea\n",
    "    def _infer_task(self, y: np.ndarray) -> str:\n",
    "        if self.task in (\"classification\", \"regression\"):\n",
    "            return self.task\n",
    "        y_type = type_of_target(y)\n",
    "        if y_type in (\"binary\", \"multiclass\"):\n",
    "            return \"classification\"\n",
    "        elif y_type in (\"continuous\",):\n",
    "            return \"regression\"\n",
    "        else:\n",
    "            return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
    "\n",
    "    # API principal\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.task_ = self._infer_task(y)\n",
    "        self._rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        max_features = self._resolve_max_features(self.n_features_in_)\n",
    "\n",
    "        self.estimators_ = []\n",
    "\n",
    "        if self.task_ == \"classification\":\n",
    "            # Aseguramos un orden global de clases para la agregación\n",
    "            self._le = LabelEncoder().fit(y)\n",
    "            self.classes_ = self._le.classes_\n",
    "\n",
    "            # Criterio por defecto si no se especifica\n",
    "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
    "\n",
    "            for _ in range(self.n_estimators):\n",
    "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
    "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
    "                tree = DecisionTreeClassifier(\n",
    "                    criterion=criterion,\n",
    "                    max_depth=self.max_depth,\n",
    "                    max_features=max_features,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "                tree.fit(Xi, yi)\n",
    "                self.estimators_.append(tree)\n",
    "\n",
    "        else:  # regresión\n",
    "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
    "\n",
    "            for _ in range(self.n_estimators):\n",
    "                Xi, yi = self._bootstrap_sample(X, y, self._rng)\n",
    "                seed = int(self._rng.randint(0, 2**31 - 1))\n",
    "                tree = DecisionTreeRegressor(\n",
    "                    criterion=criterion,\n",
    "                    max_depth=self.max_depth,\n",
    "                    max_features=max_features,\n",
    "                    random_state=seed,\n",
    "                )\n",
    "                tree.fit(Xi, yi)\n",
    "                self.estimators_.append(tree)\n",
    "\n",
    "        return self\n",
    "    # Predicción\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if not self.estimators_:\n",
    "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        if self.task_ == \"classification\":\n",
    "            # Promedio de probabilidades con alineación de clases\n",
    "            proba = self.predict_proba(X)\n",
    "            # argmax sobre el eje de clases y deshacer codificación\n",
    "            y_ind = np.argmax(proba, axis=1)\n",
    "            return self.classes_[y_ind]\n",
    "        else:\n",
    "            # Promedio de predicciones de regresión\n",
    "            preds = np.column_stack([est.predict(X) for est in self.estimators_])\n",
    "            return preds.mean(axis=1)\n",
    "    # Probabilidades para clasificación\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Probabilidades promedio para clasificación. No disponible para regresión.\"\"\"\n",
    "        if self.task_ != \"classification\":\n",
    "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes_)\n",
    "        proba_sum = np.zeros((n_samples, n_classes), dtype=float)\n",
    "\n",
    "        for est in self.estimators_:\n",
    "            # Probabilidades del árbol actual \n",
    "            est_proba = est.predict_proba(X)  \n",
    "            est_classes = est.classes_         \n",
    "            # Mapear columnas a índice global de clases\n",
    "            # Dado que LabelEncoder ordena y las clases_ del árbol también están ordenadas,\n",
    "            # podemos ubicar con searchsorted.\n",
    "            idx_map = np.searchsorted(self.classes_, est_classes)\n",
    "            # Sumar en las columnas correspondientes\n",
    "            proba_sum[:, idx_map] += est_proba\n",
    "\n",
    "        # Promedio\n",
    "        proba_avg = proba_sum / float(len(self.estimators_))\n",
    "\n",
    "        # Normalización por si algún árbol omitió clases\n",
    "        row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
    "        # Evitar división por cero: si alguna fila suma 0, asignar uniforme\n",
    "        zero_rows = (row_sums[:, 0] == 0.0)\n",
    "        if np.any(zero_rows):\n",
    "            proba_avg[zero_rows, :] = 1.0 / n_classes\n",
    "            row_sums = proba_avg.sum(axis=1, keepdims=True)\n",
    "        proba_avg /= row_sums\n",
    "\n",
    "        return proba_avg\n",
    "\n",
    "    # Compatibilidad mínima con API sklearn\n",
    "    def get_params(self, deep: bool = True):\n",
    "        return {\n",
    "            \"n_estimators\": self.n_estimators,\n",
    "            \"max_features\": self.max_features,\n",
    "            \"criterion\": self.criterion,\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"task\": self.task,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fc70",
   "metadata": {},
   "source": [
    "### Elemento 1 - Preguntas teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769e4e8",
   "metadata": {},
   "source": [
    "#### ¿Por qué el bagging ayuda a reducir la varianza del modelo?\n",
    "\n",
    "El *bagging* reduce la varianza mediante la *agregación de predicciones de múltiples modelos entrenados independientemente*. Los árboles de decisión individuales, especialmente si son profundos, son modelos de *alta varianza*; y tienden a sobreajustarse al ruido de su conjunto de entrenamiento.\n",
    "\n",
    "Al entrenar cada árbol ($h_t$) en una muestra *bootstrap* diferente, cada árbol aprende patrones ligeramente distintos y comete errores diferentes. Al final, el *bagging* combina sus predicciones ya sea por voto mayoritario en clasificación o promedio en regresión.\n",
    "\n",
    "Este proceso de promediar las predicciones de muchos modelos decorrelacionados provoca que *los errores aleatorios individuales se anulen*. El resultado es un modelo de ensamble final que mantiene el bajo sesgo de los árboles individuales, pero con una varianza mucho menor.\n",
    "\n",
    "#### ¿Qué efecto tiene limitar el número de variables consideradas en cada división?\n",
    "\n",
    "Este mecanismo es la contribución clave que distingue a un *Random Forest* de un ensamble de *bagging* simple, ya que su efecto principal es *decorrelacionar los árboles del ensamble*.\n",
    "\n",
    "Si no se limitaran las variables, todos los árboles tenderían a elegir la misma característica \"más fuerte\" en la primera división, y luego la siguiente mejor, y así sucesivamente. Esto haría que todos los árboles fueran estructuralmente muy similares, y por tanto, altamente correlacionados.\n",
    "\n",
    "Al forzar a cada nodo a elegir la mejor división solo dentro de un *subconjunto aleatorio de características* (p. ej., `max_features='sqrt'`), se incrementa la diversidad estructural del bosque. Árboles más diversos producen un ensamble más robusto y preciso cuando sus votos se combinan.\n",
    "\n",
    "#### ¿Cómo cambia el desempeño al incrementar el número de árboles en el ensamble?\n",
    "\n",
    "Generalmente, al incrementar el número de árboles (`n_estimators`), el desempeño del modelo *mejora y converge*.\n",
    "\n",
    "* Con pocos árboles, el ensamble es inestable y la varianza de la predicción es alta.\n",
    "* A medida que se añaden más árboles, el voto/promedio se vuelve más robusto, el error de generalización converge y la varianza del ensamble disminuye.\n",
    "\n",
    "A diferencia de otros métodos de ensamble, un Random Forest *no sufre de sobreajuste por añadir demasiados árboles*. El desempeño simplemente llega a un punto de equilibrio. El único costo de añadir más árboles más allá de ese punto es un mayor tiempo de entrenamiento y predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017feed",
   "metadata": {},
   "source": [
    "## Elemento 2 - Comparativa con scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo alternativo usando RandomForest de sklearn directamente\n",
    "\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "\n",
    "class SimpleRandomForestSK:\n",
    "    \"\"\"\n",
    "    Envoltura ligera sobre RandomForest de scikit-learn con una interfaz simple y consistente.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    n_estimators : int, default=100\n",
    "        Número de árboles.\n",
    "    max_features : {'sqrt','log2','all'} o int o float en (0,1], default='sqrt'\n",
    "        Subconjunto de características por división. 'all' -> usa todas (None en sklearn).\n",
    "    criterion : str, default='gini' (clasificación) o 'squared_error' (regresión)\n",
    "        Criterio del árbol base.\n",
    "    max_depth : int o None, default=None\n",
    "        Profundidad máxima de cada árbol.\n",
    "    random_state : int o None, default=None\n",
    "        Semilla de aleatoriedad.\n",
    "    task : {'auto','classification','regression'}, default='auto'\n",
    "        Tipo de problema; si 'auto', se infiere de y.\n",
    "\n",
    "    Atributos (tras fit)\n",
    "    --------------------\n",
    "    estimator_ : RandomForestClassifier o RandomForestRegressor\n",
    "        Modelo entrenado.\n",
    "    task_ : str\n",
    "        'classification' o 'regression'.\n",
    "    classes_ : np.ndarray (solo clasificación)\n",
    "        Clases del modelo.\n",
    "    n_features_in_ : int\n",
    "        Número de columnas de X usadas en el entrenamiento.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        max_features: Union[str, int, float] = \"sqrt\",\n",
    "        criterion: Optional[str] = None,\n",
    "        max_depth: Optional[int] = None,\n",
    "        random_state: Optional[int] = None,\n",
    "        task: str = \"auto\",\n",
    "    ):\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.task = task\n",
    "\n",
    "        self.estimator_ = None\n",
    "        self.task_ = None\n",
    "        self.classes_ = None\n",
    "        self.n_features_in_ = None\n",
    "\n",
    "    # Utilidades internas\n",
    "    def _resolve_max_features(self, n_features: int):\n",
    "        \"\"\"Traduce 'all' a None y valida enteros/floats dentro de rango.\"\"\"\n",
    "        mf = self.max_features\n",
    "        if mf == \"all\":\n",
    "            return None\n",
    "        if isinstance(mf, (int, np.integer)):\n",
    "            if 1 <= mf <= n_features:\n",
    "                return int(mf)\n",
    "            raise ValueError(\"max_features int debe estar en [1, n_features].\")\n",
    "        if isinstance(mf, float):\n",
    "            if 0.0 < mf <= 1.0:\n",
    "                return float(mf)\n",
    "            raise ValueError(\"max_features float debe estar en (0, 1].\")\n",
    "        if mf in (None, \"sqrt\", \"log2\"):\n",
    "            return mf\n",
    "        raise ValueError(\"max_features debe ser 'sqrt', 'log2', 'all', None, int o float en (0,1].\")\n",
    "    # Inferencia de tarea\n",
    "    def _infer_task(self, y: np.ndarray) -> str:\n",
    "        if self.task in (\"classification\", \"regression\"):\n",
    "            return self.task\n",
    "        y_type = type_of_target(y)\n",
    "        if y_type in (\"binary\", \"multiclass\"):\n",
    "            return \"classification\"\n",
    "        elif y_type in (\"continuous\",):\n",
    "            return \"regression\"\n",
    "        # fallback heurístico\n",
    "        return \"classification\" if np.issubdtype(np.asarray(y).dtype, np.integer) else \"regression\"\n",
    "\n",
    "    # API principal\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X debe ser 2D (n_samples, n_features).\")\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X y y deben tener el mismo número de filas.\")\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.task_ = self._infer_task(y)\n",
    "        max_features = self._resolve_max_features(self.n_features_in_)\n",
    "\n",
    "        if self.task_ == \"classification\":\n",
    "            criterion = self.criterion if self.criterion is not None else \"gini\"\n",
    "            self.estimator_ = RandomForestClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_depth=self.max_depth,\n",
    "                max_features=max_features,\n",
    "                criterion=criterion,\n",
    "                bootstrap=True,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        else:\n",
    "            criterion = self.criterion if self.criterion is not None else \"squared_error\"\n",
    "            self.estimator_ = RandomForestRegressor(\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_depth=self.max_depth,\n",
    "                max_features=max_features,\n",
    "                criterion=criterion,\n",
    "                bootstrap=True,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "        self.estimator_.fit(X, y)\n",
    "\n",
    "        if self.task_ == \"classification\":\n",
    "            self.classes_ = self.estimator_.classes_\n",
    "\n",
    "        return self\n",
    "    # Predicción\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.estimator_ is None:\n",
    "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
    "        X = np.asarray(X)\n",
    "        return self.estimator_.predict(X)\n",
    "    # Probabilidades para clasificación\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.estimator_ is None:\n",
    "            raise RuntimeError(\"El modelo no está entrenado. Llama a fit(X, y) primero.\")\n",
    "        if self.task_ != \"classification\":\n",
    "            raise AttributeError(\"predict_proba solo está disponible para clasificación.\")\n",
    "        X = np.asarray(X)\n",
    "        return self.estimator_.predict_proba(X)\n",
    "\n",
    "    # Compatibilidad mínima con API sklearn\n",
    "    def get_params(self, deep: bool = True):\n",
    "        return {\n",
    "            \"n_estimators\": self.n_estimators,\n",
    "            \"max_features\": self.max_features,\n",
    "            \"criterion\": self.criterion,\n",
    "            \"max_depth\": self.max_depth,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"task\": self.task,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e64602",
   "metadata": {},
   "source": [
    "### === SimpleRandomForest (propio) en Iris (clean) ===\n",
    "\n",
    "**Accuracy:** 0.889\n",
    "\n",
    "**Clases (orden interno):** `[0 1 2]`\n",
    "\n",
    "**Matriz de confusión:**  \n",
    "[[12 0 0]  \n",
    "[ 0 10 2]  \n",
    "[ 0 2 10]]  \n",
    "\n",
    "**Reporte de clasificación:**\n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|:--- |:--- |:--- |:--- |:--- |\n",
    "| **0** | 1.000 | 1.000 | 1.000 | 12 |\n",
    "| **1** | 0.833 | 0.833 | 0.833 | 12 |\n",
    "| **2** | 0.833 | 0.833 | 0.833 | 12 |\n",
    "| | | | | |\n",
    "| **accuracy** | | | 0.889 | 36 |\n",
    "| **macro avg** | 0.889 | 0.889 | 0.889 | 36 |\n",
    "| **weighted avg**| 0.889 | 0.889 | 0.889 | 36 |\n",
    "\n",
    "\n",
    "### === RandomForest (sklearn wrapper) en Iris (clean) ===\n",
    "\n",
    "**Accuracy:** 0.917\n",
    "\n",
    "**Clases (orden interno):** `[0 1 2]`\n",
    "\n",
    "**Matriz de confusión:**\n",
    "[[12 0 0]  \n",
    "[ 0 10 2]  \n",
    "[ 0 1 11]]\n",
    "\n",
    "**Reporte de clasificación:**\n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|:--- |:--- |:--- |:--- |:--- |\n",
    "| **0** | 1.000 | 1.000 | 1.000 | 12 |\n",
    "| **1** | 0.909 | 0.833 | 0.870 | 12 |\n",
    "| **2** | 0.846 | 0.917 | 0.880 | 12 |\n",
    "| | | | | |\n",
    "| **accuracy** | | | 0.917 | 36 |\n",
    "| **macro avg** | 0.918 | 0.917 | 0.917 | 36 |\n",
    "| **weighted avg**| 0.918 | 0.917 | 0.917 | 36 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c15c0a",
   "metadata": {},
   "source": [
    "### Elemento 2 - Preguntas teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82f1a7",
   "metadata": {},
   "source": [
    "#### ¿Qué diferencias cuantitativas y cualitativas se observan entre tu implementación y la de sklearn?\n",
    "\n",
    "* **Diferencia Cuantitativa:** La implementación de `scikit-learn` obtuvo un desempeño superior en el conjunto de prueba. `sklearn` alcanzó un **Accuracy de 0.917**, mientras que nuestra implementación `SimpleRandomForest` logró un **Accuracy de 0.889**.  \n",
    "\n",
    "* **Diferencia Cualitativa:** Nuestra implementación `SimpleRandomForest` captura correctamente la lógica fundamental del *bagging*: un *loop* en Python que crea una muestra *bootstrap* (`_bootstrap_sample`) y entrena un `DecisionTreeClassifier` base en cada iteración. La versión de `sklearn`, por otro lado, es una implementación nativa altamente optimizada que gestiona la paralelización (reflejado en el uso de `n_jobs=-1`), la asignación de memoria y la aleatoriedad de forma mucho más eficiente.\n",
    "\n",
    "#### ¿Cómo influyen los parámetros `n_estimators` y `max_features` en el desempeño del modelo?\n",
    "\n",
    "Ambos hiperparámetros son cruciales debido a que controlan el balance sesgo-varianza del ensamble:\n",
    "\n",
    "* `n_estimators` (Número de árboles): Controla la **robustez y estabilidad del ensamble**. Un valor mayor (ej. 100, 200) reduce la varianza del ensamble y estabiliza la predicción, hasta que el error converge.  \n",
    "\n",
    "* `max_features` (Variables por división): Controla la **diversidad del ensamble**. Un valor pequeño por ejemplo ```sqrt``` produce árboles muy diferentes entre sí, lo que produce baja correlación, lo cual es ideal para reducir la varianza del ensamble. Un valor grande por ejemplo ```all, None```hace que los árboles sean muy correlacionados, pero individualmente con menos sesgo. El valor óptimo es un balance entre ambos.\n",
    "\n",
    "#### ¿Por qué el modelo de `sklearn` suele ser más rápido o más preciso?\n",
    "\n",
    "* **Rapidez:** Principalmente por la **paralelización** y la **optimización de bajo nivel**. El *wrapper* `SimpleRandomForestSK` utiliza `n_jobs=-1`, lo que permite a `sklearn` entrenar los múltiples árboles del bosque en paralelo. La implementación `SimpleRandomForest` entrena cada árbol secuencialmente dentro de un *loop* de Python. Además, el código de `sklearn` está optimizado en Cython, siendo mucho más rápido que instanciar y entrenar repetidamente objetos de Python.  \n",
    "\n",
    "* **Precisión:** La ligera ventaja en precisión (0.917 vs 0.889) puede deberse a múltiples optimizaciones internas: heurísticas de división más avanzadas, un manejo diferente de la aleatoriedad para la selección de características, o diferencias sutiles en cómo se agregan las predicciones, por ejemplo nuestra implementación `SimpleRandomForest` usa `predict_proba` y `argmax` para `predict`, mientras que `sklearn` implementa un voto mayoritario directo más optimizado, entre otras posibles causas.\n",
    "\n",
    "#### ¿Tu implementación mantiene el mismo comportamiento al modificar la semilla aleatoria?\n",
    "\n",
    "No. Si se modifica el `random_state`, nuestra implementación (`SimpleRandomForest`) generará un ensamble completamente diferente.\n",
    "\n",
    "La semilla (`random_state`) controla dos procesos aleatorios críticos en nuestro código:\n",
    "1.  La generación de las muestras *bootstrap* (a través de `_bootstrap_sample`).\n",
    "2.  La semilla individual que se pasa a cada `DecisionTreeClassifier`, que a su vez controla la aleatoriedad en la selección de `max_features` en cada división.\n",
    "\n",
    "Cambiar la semilla resultará en un modelo distinto, y es esperable que el *accuracy* varíe ligeramente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8832f4",
   "metadata": {},
   "source": [
    "## Elemento 3 - Creación y despliegue de la API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba53cf4",
   "metadata": {},
   "source": [
    "Para realizar el despliegue no se necesitó escribir código ya que render se encarga de eso, sin embargo para la creación de la API se debe definir un módulo main.py que orqueste las peticiones, dentro de ese modulo se coloca el siguiente código que crea el objeto de la API, define y maneja los endpoints.\n",
    "\n",
    "Este código por si solo no hace nada si se ejecuta en este notebook, pero es indispensable al momento de desplegar la API.\n",
    "\n",
    "Adicionalmente para realizar las predicciones con nuestro modelo se definió un modulo srf_model.py que contiene la clase del modelo personalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, conlist\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import sys\n",
    "import app.srf_model as srf_model # Importa el módulo del modelo personalizado\n",
    "\n",
    "app = FastAPI(title=\"Random Forest API\", version=\"1.0.0\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"model/srf_propio_model.pkl\")\n",
    "\n",
    "try:\n",
    "    sys.modules['__main__'] = srf_model\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo cargar el modelo desde {MODEL_PATH}: {e}\")\n",
    "\n",
    "# Esquemas de entrada/salida\n",
    "class PredictRequest(BaseModel):\n",
    "    features: conlist(float, min_length=1) # type: ignore\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    prediction: str\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.get(\"/info\")\n",
    "def info():\n",
    "    return {\n",
    "        \"team\": \"GPT-4o mini\",\n",
    "        \"model\": type(model).__name__,\n",
    "        \"n_estimators\": getattr(model, \"n_estimators\", None),\n",
    "        \"max_depth\": getattr(model, \"max_depth\", None),\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictResponse)\n",
    "def predict(req: PredictRequest):\n",
    "    try:\n",
    "        X = np.array(req.features, dtype=float).reshape(1, -1)\n",
    "        pred = model.predict(X)[0]\n",
    "        iris_map = {0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"}\n",
    "        if isinstance(pred, (int, np.integer)):\n",
    "            pred = iris_map.get(pred, str(pred))\n",
    "        return {\"prediction\": str(pred)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Solicitud inválida: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04343311",
   "metadata": {},
   "source": [
    "### Elemento 3 - Preguntas teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eb7a4",
   "metadata": {},
   "source": [
    "#### ¿Qué ventajas ofrece exponer un modelo como servicio web?\n",
    "\n",
    "Exponer un modelo como un servicio web (API) es la práctica estándar en un entorno de producción. La ventaja principal es el **desacoplamiento**:\n",
    "\n",
    "* **Accesibilidad Universal:** Cualquier aplicación de tipo móvil, web, otro *backend*, sin importar el lenguaje en que esté escrita, puede consumir el modelo simplemente realizando una petición HTTP.  \n",
    "\n",
    "* **Mantenimiento Centralizado:** Se puede actualizar, reentrenar y desplegar una nueva versión del modelo en el servidor de API (actualizando el `model.pkl`) sin necesidad de actualizar o redistribuir las aplicaciones cliente que lo consumen.  \n",
    "\n",
    "* **Escalabilidad Independiente:** El servicio del modelo puede escalarse independientemente de la aplicación principal, permitiendo gestionar picos de demanda de inferencia de forma eficiente.\n",
    "\n",
    "#### ¿Qué riesgos o limitaciones pueden surgir si no se valida correctamente la entrada del usuario?\n",
    "\n",
    "La validación de entrada es una medida de seguridad y robustez fundamental. Si no se valida, los riesgos pueden ser:\n",
    "\n",
    "* **Fallas del Servicio o Crashes:** Si el modelo espera 4 *features* y la API recibe 3, o recibe texto en lugar de números, el *script* de predicción fallará (ej. `ValueError`). Esto causará un error 500 y potencialmente interrumpirá el servicio para todos los usuarios.  \n",
    "\n",
    "* **Predicciones Erróneas o Silenciosas:** El modelo puede recibir datos en un orden incorrecto, o con valores físicamente imposibles, por ejemplo una longitud de pétalo de -100. El modelo podría no fallar, pero devolvería una predicción absurda.  \n",
    "\n",
    "* **Riesgos de Seguridad:** En casos más complejos, entradas malformadas podrían explotar vulnerabilidades de *deserialización* en caso de usar `pickle` de forma insegura o de inyección de código.\n",
    "\n",
    "#### ¿Por qué es importante incluir un endpoint de `/health` en una API?\n",
    "\n",
    "El *endpoint* `/health` es una herramienta muy importante de **monitoreo y orquestación**.\n",
    "\n",
    "Permite a sistemas automatizados verificar constantemente que el servicio no solo está *corriendo*, sino que está *saludable*, es decir, responde activamente a peticiones y está listo para operar.\n",
    "\n",
    "Si el *health check* falla, el orquestador puede automáticamente reiniciar el servicio o dejar de enviarle tráfico, para garantizar la fiabilidad y la disponibilidad del sistema para los usuarios finales.\n",
    "\n",
    "#### ¿Cómo podrías garantizar que tu servicio mantenga disponibilidad bajo diferentes condiciones?\n",
    "\n",
    "Para garantizar la \"alta disponibilidad\" podríamos aplicar las siguientes acciones:\n",
    "\n",
    "1.  **Redundancia y Balanceo de Carga:** Desplegar múltiples instancias idénticas del servicio API detrás de un *balanceador de carga*. Si una instancia falla, el balanceador redirige el tráfico a las instancias saludables.\n",
    "\n",
    "2.  **Manejo de Carga Concurrente:** Utilizar un servidor web asíncrono que pueda manejar muchas peticiones simultáneas eficientemente.\n",
    "\n",
    "3.  **Validación Robusta de Entradas:** Asegurar que la API maneje las peticiones malformadas sin fallar o \"crasheando\".\n",
    "\n",
    "4.  **Monitoreo y Alertas:** Usar el *endpoint* `/health` para monitorear activamente el servicio y alertar en caso de que falle.\n",
    "\n",
    "5.  **Manejo de Recursos:** Asegurar que el servicio tenga suficiente memoria y CPU, y que no entre en modo \"sleep\" inesperadamente, en caso de estar utilizando la versión gratuita de Render."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7mo (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
